{
  
    
        "post0": {
            "title": "Linear Regression - Bayesian Perspective with Python and Tensorflow Probability",
            "content": "Linear regression is one of the basic predictive analysis technique, which models the linear relationship between predictor and the target(continuous variable). With the advent of more robust techniqe like neural networks or tree based methods like random forest/xgboost, linear regression is seldom used now directly. But the concept of linear regression forms the fundamental building blocks of the most sophasticated models and this article provides a different aspect on linear regression based on bayesian technique. Fundamentally, we are going to look at how the loss function is constructed for linear regression which has a probabilistic connection and alternate ways to solve the linear regression problem. . This article assumes that the reader is familiar with linear regression and gradient descent and so prior understanding of the concepts would be much helpful.I would be using very simple examples to make the article relevant and easy to understand. . Linear Regression - Gradient Descent . Just to set the context for the rest of the article, we will quickly look into what a linear regression is and how it learns the parameter using gradient descent at a high level. As you know, the linear regression models a linear relationship between the predictors (X1,X2,..) and the target (Y) using a set of parameters (w1,w2,..) . $$ begin{equation*} yhat = w1*X1 + w2*X2 + b end{equation*} $$The main objective is to reduce the SSE(Sum of squared errors between) the predicted value y_hat and the target Y which is nothing but the loss function for linear regeression . $$ begin{equation*} loss = frac{1}{2m} sum_{i=1}^n (yhat -y)^2 end{equation*} $$ The following code snippet shows the way the model parameters are updated by computing the gradient of loss function with respect to the parameters. We randomly generate an example dataset of x and relate it to y by a function y = 450x+e where e is some random noise with mean zero. Here x is the predictor variable and y is the target variable which are the input to the model and we need to find the values of w ( in our case its 450. we are going to take this as reference and find out if the model is able to predict the parameters correctly) . import numpy as np from matplotlib import pyplot as plt from numba import jit x= np.random.rand(2000)*100 w=450 e = np.random.normal(0.0,4000.0,2000) y=(w*x)+e plt.scatter(x,y) . &lt;matplotlib.collections.PathCollection at 0x7fdfe94331d0&gt; . To start with, we initialize the w and b to a random point. In each iteration, we update the model parameters by calculating the gradient of the loss function as below. . gradient: begin{equation*} dw = frac{1}{m} sum_{i=1}^n (yhat_i -y_i)*x_i end{equation*} . update: begin{equation*} w = w - lr*dw end{equation*} . #LR w0 = np.random.rand(1)[0]*100 lr =0.00001 m =len(x) for i in range(100000): y_hat = (w0*x) loss = (1/(2*m))*np.sum(np.square(y_hat-y)) dw =(1/m)*np.sum((y_hat-y)*x) w0 = w0-(lr*dw) print(f&quot;w: {w0}&quot;) . w: 449.0120482723969 . As you can see, the predicted model parameters are close to our reference values(450) . Loss Function Decoded . To understand the loss function of linear regression, we will first review about the the gaussian distribution. The equation of gaussian distribution is given by, . begin{equation*} X sim mathcal{N}( mu, , sigma^{2}) end{equation*} begin{equation*} pdf(X=x) = frac{1}{ sqrt{2 pi sigma^{2}} }e^{ frac{-(x- mu)^2}{ 2 sigma^{2}} } end{equation*}The following plots shows a gaussian distribution with mean 30 and standard deviation 10. The second and third plot is the log and negative log of the given distribution. As you can see from the plots, that the negative log gaussian is a convex function. . import seaborn as sns import scipy.stats fig,ax = plt.subplots(1,3,figsize=(15,5)) gaussian = scipy.stats.norm(30,10) gx=np.linspace(1,60,100) gy=gaussian.pdf(gx) log_gy = np.log(gy) neg_log_gy = -log_gy ax[0].set_title(&quot;gaussian &quot;) ax[0].plot(gx,gy) ax[1].set_title(&quot;log gaussian &quot;) ax[1].plot(gx,log_gy) ax[2].set_title(&quot;negative log gaussian &quot;) ax[2].plot(gx,neg_log_gy) . /usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead. import pandas.util.testing as tm . [&lt;matplotlib.lines.Line2D at 0x7fdfe2872e10&gt;] . Next, we will look at Maximum likelihood estimation, which sets the probabilistic base for estimating the model parameters w . The bayes theorem states that, . begin{equation*} p(x|y) = frac{p(y|x) p(x)}{p(y)} end{equation*} begin{equation*} p(x|y) propto p(y|x) p(x) end{equation*} begin{equation*} posterior propto likelihood * prior end{equation*}The posterior is the unknown that we need to model, and for the linear regression, the unknown is the parameter w with the target y and predictor x. so we are going to rewrite the equation as . begin{equation*} p(w|y;x) propto p(y|w;x) * p(w) end{equation*}Assuming a uniform prior, it is always constant and we can ignore it from the equation. . so, begin{equation*} p(w|y;x) propto p(y|w;x) end{equation*} . For uniform prior, the posterior is directly proportional to likelihood and so , the estimate of w is the value of w that maximises the likelihood and hence it is called maximum likelihood estimation. For linear regression, the likelihood is modelled as gaussian distribution with mean wx+b and constant variance and we need to find the value of w that maximises this gaussian or minimize the negative log of gaussian. Let us derive the negative log gaussian for linear regression, . begin{equation*} p(y|w;x) sim mathcal{N}( mu, , sigma^{2}) end{equation*} begin{equation*} mu = wx+b = yhat end{equation*} begin{equation*} p(y|w;x) = frac{1}{ sqrt{2 pi sigma^{2}} }e^{ frac{-(y-yhat)^2}{ 2 sigma^{2}} } end{equation*}Taking the log of the above equation . begin{equation*} log ( p(y|w;x) ) = log( frac{1}{ sqrt{2 pi sigma^{2}} })+ frac{-(y-yhat)^2}{ 2 sigma^{2}} end{equation*}since we are maximizing the likelihood, we can ignore the constants, begin{equation*} log ( p(y|w;x) ) = - frac{1}{2}(y-yhat)^2 end{equation*} . taking neagtive of the above equation, begin{equation*} neg(log ( p(y|w;x) )) = frac{1}{2}(y-yhat)^2 end{equation*} . MSE begin{equation*} neg(log ( p(y|w;x) )) = frac{1}{2m} sum_{i=1}^n (yhat -y)^2 end{equation*} . As you can see, this is the loss function of linear regression which is nothing but the negative log likelihood of gaussian distribution with mean wx+b and constant variance and from the plot above, we know that the negative log of the gaussian is convex. So when we are trying to optimize the mean square error for linear regression using gradient descent, we are optimizing the negative log of gaussian and since it is convex, we are guaranteed to find the global optima. . Linear regression using MCMC . To learn the model parameters, we optimize on the loss function until convergence using gradient descent. But gradient descent is not the only way to find the parameters and there are other techniques available. We will look at one other way to find the model parameters using MCMC - Markov Chain Monte Carlo. MCMC is usally used to sample from intractable distribution and often used as approximate solution and for the cases like linear regression where the distribution is well defined, optimization using gradient descent gives good result. The MCMC is described here to provide an alternate view on inferencing from posterior distribution. . The idea of MCMC is to build a markov chain whose stationary distribution is the posterior using algorithms like gibbs sampling or metropolis-hasting. Without going in detail of MCMC, the method samples from the posterior distribution (likelihood * prior) as specified in joint_log_prob function in the code snippet below. The method repetedly samples the values of parameters from the posterior distribution and collects them.Unlike Maximum likelihood estimation where we get the single value of parameters, the MCMC provides us with the distibution of model parameters. The plot below shows the distribution of collected samples. . # Probabilistic perspective . import tensorflow_probability as tfp import tensorflow as tf tfd = tfp.distributions tfb = tfp.bijectors def joint_log_prob(y,x,w): rv_w = tfd.Normal(0.,500.) yhat = tf.multiply(w,x) likelihood = tfd.Normal(yhat,10.) diff = y-y_hat return (rv_w.log_prob(w) + tf.reduce_mean(likelihood.log_prob(y))) number_of_steps = 50000 burnin = 5000 leapfrog_steps=5 # Set the chain&#39;s start state. initial_chain_state = [ 200.* tf.ones([], dtype=tf.float32, name=&quot;init_prob_w&quot;) ] # Since HMC operates over unconstrained space, we need to transform the # samples so they live in real-space. unconstraining_bijectors = [ tfp.bijectors.Identity() ] # Define a closure over our joint_log_prob. # The closure makes it so the HMC doesn&#39;t try to change the `occurrences` but # instead determines the distributions of other parameters that might generate # the `occurrences` we observed. unnormalized_posterior_log_prob = lambda *args: joint_log_prob(y,x, *args) # Initialize the step_size. (It will be automatically adapted.) step_size = tf.Variable(0.3, dtype=tf.float32) # Defining the HMC hmc = tfp.mcmc.TransformedTransitionKernel( inner_kernel=tfp.mcmc.HamiltonianMonteCarlo( target_log_prob_fn=unnormalized_posterior_log_prob, num_leapfrog_steps=leapfrog_steps, step_size=0.3, #step_size_update_fn=tfp.mcmc.make_simple_step_size_update_policy(num_adaptation_steps=2000), state_gradients_are_stopped=True), bijector=unconstraining_bijectors) # Sampling from the chain. [ posterior_prob_w ], kernel_results = tfp.mcmc.sample_chain( num_results=number_of_steps, num_burnin_steps=burnin, current_state=initial_chain_state, kernel=hmc) # Initialize any created variables. #init_g = tf.global_variables_initializer() #init_l = tf.local_variables_initializer() . /usr/local/lib/python3.6/dist-packages/tensorflow_probability/python/mcmc/sample.py:333: UserWarning: Tracing all kernel results by default is deprecated. Set the `trace_fn` argument to None (the future default value) or an explicit callback that traces the values you are interested in. warnings.warn(&#34;Tracing all kernel results by default is deprecated. Set &#34; . posterior_prob_w.numpy() . array([449.04984, 449.04984, 449.04984, ..., 449.02338, 448.80603, 448.80603], dtype=float32) . plt.hist(posterior_prob_w.numpy()[5000:]) . (array([ 18., 279., 2084., 7567., 13677., 13320., 6246., 1557., 231., 21.]), array([448.28525, 448.4328 , 448.58035, 448.72794, 448.8755 , 449.02304, 449.1706 , 449.31815, 449.46573, 449.61328, 449.76083], dtype=float32), &lt;a list of 10 Patch objects&gt;) . np.mean(posterior_prob_w.numpy()[5000:]) . 449.0111 . xax = [i for i in range(1, len(posterior_prob_w))] plt.scatter(xax,posterior_prob_w.numpy()[1:]) . &lt;matplotlib.collections.PathCollection at 0x7fdfd9dfe710&gt; . np.std(posterior_prob_w.numpy()[5000:]) . 0.17534533 .",
            "url": "https://vignesh8491.github.io/myblog/machine%20learning/tensorflow/probability/python/bayesian/2020/07/19/linear_regression_blog.html",
            "relUrl": "/machine%20learning/tensorflow/probability/python/bayesian/2020/07/19/linear_regression_blog.html",
            "date": " â€¢ Jul 19, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": ". Languages and Frameworks: . &nbsp; &nbsp; &nbsp; .",
          "url": "https://vignesh8491.github.io/myblog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}